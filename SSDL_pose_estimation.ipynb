{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Pose Estimation\n",
        "\n",
        "In this script, we will apply Tensorflow's pose estimation model [MoveNet Thunder](https://www.tensorflow.org/hub/tutorials/movenet) to two short videos of two speeches in the German Bundestag. The videos of those speeches are stored in a GitHub repository under the following link: [github.com/orittmann/ssdl_body_language.git](https://github.com/orittmann/ssdl_body_language.git). We start by loading the content of this repository into our environment:"
      ],
      "metadata": {
        "id": "5fkIJ69TsnJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/orittmann/ssdl_body_language.git"
      ],
      "metadata": {
        "id": "h9ZeIAn9p5eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once this code run succesfully, you should be able to find all files of the repository, including its folder structure, in this Colab notebook. The two videos that we would like to analyze are located at `ssdl_body_language/videos/speech1_gabriela_heinrichs.mp4` and `ssdl_body_language/videos/speech2_klaus_ernst.mp4`.\n",
        "\n",
        "The repository also includes the file `ssdl_body_language/model/lite-model_movenet_singlepose_thunder_3.tflite`. This is the pre-trained pose estimation model that we will use to analyze the videos. You can find more details [here](https://www.tensorflow.org/hub/tutorials/movenet).\n",
        "\n",
        "Now that we have our files in place, we make sure to load all necessary dependencies:"
      ],
      "metadata": {
        "id": "sVFtNRFAbcFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# to store resulting data as json\n",
        "import json\n",
        "\n",
        "# to get filenames in directory\n",
        "import os\n",
        "import fnmatch"
      ],
      "metadata": {
        "id": "ggmeKedTbKrd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we load and prepare the pose estimation model:"
      ],
      "metadata": {
        "id": "osA5T5Qkb3DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path='ssdl_body_language/model/lite-model_movenet_singlepose_thunder_3.tflite')\n",
        "interpreter.allocate_tensors()"
      ],
      "metadata": {
        "id": "4PUIfUVJb5lL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up the function `make_keypoint_detection`, which processes an input video frame-by-frame and performs keypoint detection using TensorFlow's MoveNet model. The function resizes each frame of the video to the required size, prepares it for the model, runs inference to get keypoints, stores the keypoints, and finally returns all the keypoints detected from the video."
      ],
      "metadata": {
        "id": "RGQUMS8DbkCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_keypoint_detection(video_path):\n",
        "\n",
        "    # \"output_images\" is an empty list that will store the keypoints detected\n",
        "    # in each frame of the video.\n",
        "    output_images = []\n",
        "\n",
        "    # \"cap\" is a video capturing object that allows us to read the frames of\n",
        "    # the video stored at video_path\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # We initialize a while-loop that that loops through all frames of the video\n",
        "    # The loop is open as long as the video capture object \"cap\" is open.\n",
        "    # That is, until all frames are analyzed.\n",
        "    while cap.isOpened():\n",
        "\n",
        "        # We read the current frame of the video\n",
        "        # \"ret\" is boolean, indicating if the frame was read successfully\n",
        "        # \"frame\" is the actual frame of the video\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # We only proceed if the frame is read correctly (i.e., if ret is TRUE)\n",
        "        if not ret:\n",
        "            print(\"Stream end.\")\n",
        "            break\n",
        "\n",
        "        # Image Preparation:\n",
        "        # We create a copy of the current frame to avoid modyfing the original.\n",
        "        img = frame.copy()\n",
        "\n",
        "        # MoveNet Thunder requires a frame size of 256x256\n",
        "        # For that reason, we resize the frame.\n",
        "        # This includes padding since the original video is not square\n",
        "        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 256, 256)\n",
        "\n",
        "        # We convert the resized image to a TensorFlow float32 tensor,\n",
        "        # so that we can feed it into the model\n",
        "        input_image = tf.cast(img, dtype=tf.float32)\n",
        "\n",
        "        # Setting Up Model Input and Output:\n",
        "        input_details = interpreter.get_input_details()\n",
        "        output_details = interpreter.get_output_details()\n",
        "\n",
        "        # Running inference:\n",
        "        # - We set up the input tensor with the prepare input frame\n",
        "        # - We run the model\n",
        "        # - We retrieve the output tensor, which contains the keypoints\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "        interpreter.invoke()\n",
        "        keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # Storing the Results:\n",
        "        # We transform numpy array to a list (this makes it easier to store as\n",
        "        # the output as a .json file later) and append it to \"output_images\"\n",
        "        # for storage\n",
        "        output_images.append(keypoints_with_scores.tolist())\n",
        "\n",
        "    # Final Steps:\n",
        "    # - We release the video capturing object\n",
        "    # - We return the list of keypoints detected in each frame of the video\n",
        "    cap.release()\n",
        "\n",
        "    return output_images"
      ],
      "metadata": {
        "id": "2cWe_XKxbx76"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply this function to two videos only. Nevertheless, we'll write the code in a way that allows us to efficiently apply it to as many videos as we want. To do so, we create a vector, `video_files`, that contains the filenames of all videos we would like to analyze."
      ],
      "metadata": {
        "id": "apkDofkbqYhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_files = fnmatch.filter(os.listdir(\"ssdl_body_language/videos\"), \"*.mp4\")\n",
        "\n",
        "print(video_files)"
      ],
      "metadata": {
        "id": "VkjdviSQqaxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and loop through this vector, applying our function to each video in `video_files`."
      ],
      "metadata": {
        "id": "WCJMgKeZqmg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over all videos\n",
        "for i in np.arange(0, len(video_files)):\n",
        "    # We start by retrieving the file path to the current video\n",
        "    current_file = video_files[i]\n",
        "    current_path = \"ssdl_body_language/videos/\" + current_file\n",
        "\n",
        "    # We print a message indicating the start of inference for the current video\n",
        "    print(\"Start inference for video \" + str(i) + \": \" + current_file)\n",
        "\n",
        "    # Executing keypoint detection:\n",
        "    # We call the \"make_keypoint_detection\" function with the path to the\n",
        "    # current video and store the resulting keypoints in \"keypoints_result_tmp\"\n",
        "    keypoints_result_tmp = make_keypoint_detection(current_path)\n",
        "\n",
        "    # Store data:\n",
        "    # We specify the filepath and file name of the output file\n",
        "    # and store the output\n",
        "    res_json_file = current_file.replace(\"mp4\", \"json\")\n",
        "    res_json_file_path = \"ssdl_body_language/movenet_results/\" + res_json_file\n",
        "\n",
        "    with open(res_json_file_path, 'w') as fp:\n",
        "        json.dump(keypoints_result_tmp, fp)\n",
        "\n",
        "    # We delete the temporary keypoint results to free up memory\n",
        "    del keypoints_result_tmp\n",
        "\n",
        "    # Finally, we print a message indicating the end of inference for the\n",
        "    # current video\n",
        "    print(\"End inference for video \" + str(i) + \": \" + current_file)\n"
      ],
      "metadata": {
        "id": "htzd3-LJqoTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now applied the pose estimation model to both videos and stored the results as `.json` files in `ssdl_body_language/movenet_results`. From here on, we will continue working with R. Navigate back to the [github repository](https://github.com/orittmann/ssdl_body_language) to find the respective script."
      ],
      "metadata": {
        "id": "LI1sv2exm1_q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYf3tMXTs5df"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}