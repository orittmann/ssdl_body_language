---
title: "Computationally Analyzing Politicians' Body Language Using Pose Estimation"
author: 
  - Oliver Rittmann
date: "October 16, 2024"
output:
  html_document:
    toc: true
    toc_float: true
    css: css/lab.css
  pdf_document:
    toc: yes
  html_notebook:
    toc: true
    toc_float: true
    css: css/lab.css
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
mainfont: SourceSansPro
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(here)
library(dplyr)
library(stringr)
library(jsonlite)
library(viridis)
library(scales)
library(imager)
```



In the second part of the workshop, we'll analyze the pose estimation output of the video sequences of two speeches, both delivered in the German Bundestag. I've chosen two speeches that differ strongly regarding the speakers' nonverbal displays. **Speech 1** by Gabriela Heinrich can be characterized as a speech with low nonverbal effort, while **speech 2** by Klaus Ernst is a speech with high nonverbal effort. Let's have a second look at what those speeches look like.

![Speech 1: Garbiela Heinrich, low nonverbal effort](videos/gifs/speech1_gabriela_heinrich.gif)

![Speech 2: Klaus Ernst, high nonverbal effort](videos/gifs/speech2_klaus_ernst.gif)

In the remainder of the workshop, we will cover five sections. 

  1. Loading pose estimation output into R
  2. Organizing pose estimation data in as a time-series data set
  3. Visualizing the pose estimation data
  4. Calculating measure of gesticulation
  5. Calculating measure of posture

# 1. Loading pose estimation (MoveNet) output into R

The pose estimation data comes in `.json` format and is stored in `movenet_results`. We have two files, one for each video.

```{r}
files <- list.files(here("movenet_results"))

json_files <- files[str_detect(files, ".json")]

json_files
```

Let's load the data into our environment and have a look at how it looks like.

```{r}
speech1_raw <- jsonlite::fromJSON(paste0("movenet_results/", json_files[1]))
speech2_raw <- jsonlite::fromJSON(paste0("movenet_results/", json_files[2]))

dim(speech1_raw)
dim(speech2_raw)
```

The dimensions are 274 x 1 x 1 x 17 x 3 -- Any ideas what these dimensions are about?

```{r}
# the first frame
speech1_raw[1,,,,]

# the second frame
speech1_raw[2,,,,]
```

The first two columns of the last dimension represents the y- and x-coordinates of the 17 body keypoints:

  - 1 nose
  - 2 left eye
  - 3 right eye
  - 4 left ear
  - 5 right ear
  - 6 left shoulder
  - 7 right shoulder
  - 8 left elbow
  - 9 right elbow
  - 10 left wrist
  - 11 right wrist
  - 12 left hip
  - 13 right hip
  - 14 left knee
  - 15 right knee
  - 16 left ankle
  - 17 right ankle

The third column tells us how confident the model was in its detection, with confidence scores ranging between 0 and 1.

# 2. Organizing pose estimation data in as a time-series data set

Let's bring this data into a format that makes it easier to work with. The following function requires as input the keypoint data as loaded in the previous step, a file name, and the length of the respective video. The function takes the data and puts it into a time-series data frame that we can work with more easily.

```{r}
array_to_timeseries <- function(data_list, 
                                file_name,
                                video_length){
  ts_dat <- 
    data.frame(file_name = rep(file_name, dim(data_list)[1]),
               frame = 1:dim(data_list)[1],
               timestamp = NA,
               matrix(NA, ncol = 17, nrow = dim(data_list)[1]),
               matrix(NA, ncol = 17, nrow = dim(data_list)[1]),
               matrix(NA, ncol = 17, nrow = dim(data_list)[1]))
  
  keypoints_x_cols <- 4:20
  keypoints_y_cols <- 21:37
  keypoints_confidence_cols <- 38:54
  names(ts_dat)[keypoints_x_cols] <- paste0("kp", 1:17, "_x")
  names(ts_dat)[keypoints_y_cols] <- paste0("kp", 1:17, "_y")
  names(ts_dat)[keypoints_confidence_cols] <- paste0("kp", 1:17, "_conf")
  
  # calculate timestamps
  framerate <- dim(data_list)[1] / video_length
  ts_dat$timestamp <- ts_dat$frame /framerate
  
  for (i in 1:dim(data_list)[1]) {
    # x-values
    ts_dat[i,keypoints_x_cols] <- data_list[i,,,,2]
    # y-values
    ts_dat[i,keypoints_y_cols] <- data_list[i,,,,1]
    # confidence values
    ts_dat[i,keypoints_confidence_cols] <- data_list[i,,,,3]
  }
  
  return(ts_dat)
}
```

Let's apply the function to our two speeches.

```{r}
speech1_keypoints <- 
  array_to_timeseries(data_list = speech1_raw,
                      file_name = json_files[1],
                      video_length = 1/25 * dim(speech1_raw)[1]) # framerate = 25fps

speech2_keypoints <- 
  array_to_timeseries(data_list = speech2_raw,
                      file_name = json_files[2],
                      video_length = 1/25 * dim(speech2_raw)[1])


```

What do we have now?

```{r}
head(speech1_keypoints)
```

The new data frame contains the following variables:

Metadata: 

  - `file_name` = name of the .json file (our data source)
  - `frame` = frame identifier, increasing number in the order of their appearance
  - `timestamp` = timestamp of the frame within the video sequence

Key point data:

  - `kp1_x` = x-coordinate of key point 1 (nose)
  - `kp2_x` = x-coordinate of key point 2 (left eye)
  - `kp3_x` = x-coordinate of key point 3 (right eye)
  - ...
  - `kp17_x` = x-coordinate of key point 17 (right ankle)
  - `kp1_y` = y-coordinate of key point 1 (nose)
  - `kp2_y` = y-coordinate of key point 2 (left eye)
  - `kp3_y` = y-coordinate of key point 3 (right eye) 
  - ...
  - `kp17_conf` = y-coordinate of key point 17 (right ankle)
  - `kp1_conf` = confidence for key point 1 (nos)
  - `kp2_conf` = confidence for key point 2 (left eye)
  - `kp3_conf` = confidence for key point 3 (right eye)
  - ...
  - `kp17_conf` = confidence for key point 17 (right ankle)


# 3. Visualizing pose estimation data

In this section we want to get a feeling for the data by looking at it visually.We want to see what the raw pose estimation data looks like and compare it to the video frames. I extracted all frames of both videos and stored them in `videos/frames_speech1`and `videos/frames_speech2`, respectively.

We are going to look at the first frame of our second video (that is, `videos/speech2_klaus_ernst.mp4`). Let's first inspect the frame itself. To plot the image in R, we start by loading the image into our environment using the `load.image()`-function provided by the `imager`-package.

```{r}
frame_number <- 1 # you can change this number to inspect other frames

frame_filename <- paste0("frame",
                         str_pad(frame_number, 5, pad = "0"),
                         ".png")

image <- load.image(paste0("videos/frames_speech2/",
                           frame_filename))
```

What is the size of this image?

```{r}
x_size <- dim(image)[1]
y_size <- dim(image)[2]

x_size
y_size
```

Plotting the image is actually quite easy:

```{r}
plot(image,
       axes = T)
```

Notice the y-axis: The origin point of the frame is in the upper left corner, with values on both axis increasing downwards (y-axis) and rightwards (x-axis). This is a convention that we need to keep in mind. You will see that it also affects our key point representations.

Next, we will have a look at the key point data of that first frame. First, we extract the keypoints of the first frame from our time series of key points.



```{r}
# We want to plot one frame, so let's get the data for one frame
keypoints <- speech2_keypoints[speech2_keypoints$frame == frame_number,]

keypoints
```

The frame only shows upper body key points. Thus, so it makes sense to subset the data to those upper body key points and discard all lower body key points.

Remember the list from above? `kp1` to `kp13` are upper body key points while `kp14` to `kp17` are lower body key points.

```{r}
# we want to subset to the upper body
upper_body_pattern <- paste0("kp", 1:13, "_", collapse = "|")
keypoints <- keypoints[str_detect(names(keypoints), upper_body_pattern)]
```


To make plotting a bit easier, we reorganize the remaining data. We want a matrix that stores the x-coordinates of all key points in the one column, and the y-coordinates in another column:

```{r}
kp_x <- t(keypoints[, str_detect(names(keypoints), "_x")])
kp_y <- t(keypoints[, str_detect(names(keypoints), "_y")])

kp_frame <- data.frame(cbind(kp_x, kp_y))
colnames(kp_frame) <- c("x", "y")

kp_frame
```


Now we are finally in a position where we can visualize the upper key points of our frame:

```{r}
plot(x = kp_frame$x,
     y = kp_frame$y,
     xlim = c(0, 1),
     ylim = c(0, 1),
     asp = 1,
     pch = 19,
     las = 1,
     xlab = "x-coordinates",
     ylab = "y-coordinates")
grid()
```

Okay, this does not look very helpful yet. Let's add lines between some of the key points to create an image that is closer to the one we've seen on the slides earlier.


```{r}

# this list stores all key points we would like to connect

edges <- list(c(1, 2),    # nose - left eye
              c(1, 3),    # nose - right eye
              c(2, 4),    # left eye - left ear
              c(3, 5),    # right eye - right ear
              c(1, 6),    # nose - left shoulder
              c(1, 7),    # nose - right shoulder
              c(6, 8),    # left shoulder - left elbow
              c(8, 10),   # left elbow - left wrist
              c(7, 9),    # right shoulder - right elbow
              c(9, 11),   # right elbow - right wrist
              c(6, 7),    # left shoulder - right shoulder
              c(6, 12),   # left shoulder - left hip
              c(7, 13),   # right shoulder - right hip
              c(12, 13),  # left hip - right hip
              c(12, 14),  # left hip - left knee
              c(14, 16),  # left knee - left ankle
              c(13, 15),  # right hip - right knee
              c(15, 17))  # right knee - right ankle


# 1) We start with an empty plot:
plot(x = kp_frame$x,
     y = kp_frame$y,
     type = "n",
     xlim = c(0, 1),
     ylim = c(0, 1),
     asp = 1,
     las = 1,
     xlab = "x-coordinates",
     ylab = "y-coordinates")
grid()

# 2) Next, we add all lines to the plot:
for (e in 1:length(edges)) {
  p1 <- kp_frame[edges[[e]][1], c("x", "y")]
  p2 <- kp_frame[edges[[e]][2], c("x", "y")]
  
  if (all(!is.na(p1)) & all(!is.na(p2))) {
    segments(
      x0 = as.numeric(p1$x),
      y0 = as.numeric(p1$y),
      x1 = as.numeric(p2$x),
      y1 = as.numeric(p2$y),
      lwd = 2,
      col = viridis(1, 0.5)
    )
  }
}

# 3) And add the key points on top:
points(x = kp_frame$x,
       y = kp_frame$y,
       pch = 19,
       cex = 1.25,
       col = viridis(1))
```

This looks better, but the figure is upside down. Why? Because, as we've seen earlier, the y-axis is reversed. We can fix this quite easily by adjusting `ylim`:

```{r}

plot(x = kp_frame$x,
     y = kp_frame$y,
     type = "n",
     xlim = c(0, 1),
     ylim = c(1, 0), # changing this from c(0,1) to c(1,0) fixes the y-axis
     asp = 1,
     las = 1,
     xlab = "x-coordinates",
     ylab = "y-coordinates")
grid()
  

# The rest of the code remains the same:

for (e in 1:length(edges)) {
  p1 <- kp_frame[edges[[e]][1], c("x", "y")]
  p2 <- kp_frame[edges[[e]][2], c("x", "y")]
  
  if (all(!is.na(p1)) & all(!is.na(p2))) {
    segments(
      x0 = as.numeric(p1$x),
      y0 = as.numeric(p1$y),
      x1 = as.numeric(p2$x),
      y1 = as.numeric(p2$y),
      lwd = 2,
      col = viridis(1, 0.5)
    )
  }
}

points(x = kp_frame$x,
       y = kp_frame$y,
       pch = 19,
       cex = 1.25,
       col = viridis(1))
```

This looks more like what we envisioned. We can also look at the frame and the key point representation side by side:



```{r}

# create two columns
par(mfrow = c(1, 2))

# plot the frame:
plot(image,
     axes = T)

# plot the key point representation
plot(x = kp_frame$x,
     y = kp_frame$y,
     asp = 1,
     xlim = c(0, 1),
     ylim = c(1, 0), # changing this from c(0,1) to c(1,0) fixes the y-axis
     axes = T,
     xlab = "",
     ylab = "",
     bty = "n",
     las = 1)
grid()


for (e in 1:length(edges)) {
  p1 <- kp_frame[edges[[e]][1], c("x", "y")]
  p2 <- kp_frame[edges[[e]][2], c("x", "y")]
  
  if (all(!is.na(p1)) & all(!is.na(p2))) {
    segments(
      x0 = as.numeric(p1$x),
      y0 = as.numeric(p1$y),
      x1 = as.numeric(p2$x),
      y1 = as.numeric(p2$y),
      lwd = 2,
      col = viridis(1, 0.5)
    )
  }
}

points(x = kp_frame$x,
       y = kp_frame$y,
       pch = 19,
       cex = 1.25,
       col = viridis(1))
```

Or, we can plot the key points on top of the frame. To do so, we need to rescale the key points. The key-points are scaled between zero and one, whereas our image are scaled according to their pixel size, i.e. 720x400.

These two functions will rescale the x- and y-values of our key point representations so that they match the scale of the frame:

```{r}
scale_x <- function(x, x_size){x*x_size}
scale_y <- function(y, x_size, y_size){y*x_size-((x_size-y_size)/2)}

kp_frame$x_scaled <- scale_x(x = kp_frame$x, x_size = x_size)
kp_frame$y_scaled <- scale_y(y = kp_frame$y, x_size = x_size, y_size = y_size)

```

```{r}
plot(image,
     axes = F)

for (e in 1:length(edges)) {
  p1 <- kp_frame[edges[[e]][1], c("x_scaled", "y_scaled")]
  p2 <- kp_frame[edges[[e]][2], c("x_scaled", "y_scaled")]
  
  if (all(!is.na(p1)) & all(!is.na(p2))) {
    segments(
      x0 = as.numeric(p1$x),
      y0 = as.numeric(p1$y),
      x1 = as.numeric(p2$x),
      y1 = as.numeric(p2$y),
      lwd = 2,
      col = viridis(2)[1]
    )
  }
}

points(x = kp_frame$x_scaled,
       y = kp_frame$y_scaled,
       pch = 19,
       cex = 1.25,
       col = viridis(2)[2])
```

I hope visualizing individual frames gave you a feeling for the data. We now proceed to calculating our two body language indicators from time series of key point representations. That is, we will calculate our indicator of gesticulation and our measure of posture for both speech sequences. 

# 4. Calculating measure of gesticulation

Let's recall how we would like to quantify the level of gesticulation shown by the speaker. We defined gesticulation as dynamic use of gestures, specifically through hand movement. We will make use of the fact that hand movement is captured by the distances of hand locations between frames. The more a speaker moves their hands, the higher the distance of hand locations between two consecutive frames. We construct our **measure of gesticulation** as the **average between-frame distance of the speaker's left and right hand**.

To calculate between-frame distances of key points, we need a function to calculate euclidean distances:

```{r}
euclidean_distance <- function(x1, y1, x2, y2){
  dist <- sqrt((x2-x1)^2 + (y2-y1)^2)
  return(dist)
}
```

With this function at hand, we can code a function that calculates the distances of the left and right wrists between all frames of a video sequence:


```{r}
calculate_wrist_movement <- function(kp_data){
  
  # LEFT WRIST (kp10)
  kp_data$left_wrist_movement <- 
      euclidean_distance(x1 = kp_data$kp10_x, 
                         y1 = kp_data$kp10_y,
                         x2 = lead(kp_data$kp10_x), 
                         y2 = lead(kp_data$kp10_y))
  
  # RIGHT WRIST (kp11)
  kp_data$right_wrist_movement <- 
      euclidean_distance(x1 = kp_data$kp11_x, 
                         y1 = kp_data$kp11_y,
                         x2 = lead(kp_data$kp11_x), 
                         y2 = lead(kp_data$kp11_y))
  
  return(kp_data)
}
```

Let's apply the function to both speeches

```{r}
speech1_keypoints <- calculate_wrist_movement(speech1_keypoints)
speech2_keypoints <- calculate_wrist_movement(speech2_keypoints)
```

The function added variables for left and right wrist movement

```{r}
head(speech1_keypoints[, c("left_wrist_movement", "right_wrist_movement")])
head(speech2_keypoints[, c("left_wrist_movement", "right_wrist_movement")])
```

We can visualize the times-series of the left and right wrist movement variables. 

```{r}
par(mfrow = c(1, 2))

# LEFT WRIST
plot(x = speech1_keypoints$frame,
     y = speech1_keypoints$left_wrist_movement,
     type = "n",
     las = 1,
     xlab = "Video Frame",
     ylab = "",
     main = "Left Wrist Movement",
     ylim = c(0, 0.2),
     xlim = c(0, 450))

grid()

# Speech 1, left wrist
lines(x = speech1_keypoints$frame,
      y = speech1_keypoints$left_wrist_movement,
      col = viridis(3)[1],
      lwd = 2)

# annotation
speech1_pos_id <- which.max(speech1_keypoints$frame)-1
text(x = speech1_keypoints$frame[speech1_pos_id],
     y = speech1_keypoints$left_wrist_movement[speech1_pos_id],
     labels = "Speech 1",
     pos = 4,
     col = viridis(3)[1])

# Speech 2, left wrist
lines(x = speech2_keypoints$frame,
      y = speech2_keypoints$left_wrist_movement,
      col = viridis(3)[2],
      lwd = 2)

# annotation
speech2_pos_id_l <- which.max(speech2_keypoints$left_wrist_movement)
text(x = speech2_keypoints$frame[speech2_pos_id_l],
     y = speech2_keypoints$left_wrist_movement[speech2_pos_id_l],
     labels = "Speech 2",
     pos = 3,
     col = viridis(3)[2])



# RIGHT WRIST
plot(x = speech1_keypoints$frame,
     y = speech1_keypoints$right_wrist_movement,
     type = "n",
     las = 1,
     xlab = "Video Frame",
     ylab = "",
     main = "Right Wrist Movement",
     ylim = c(0, 0.2),
     xlim = c(0, 450))

grid()

# Speech 1, right wrist
lines(x = speech1_keypoints$frame,
      y = speech1_keypoints$right_wrist_movement,
      col = viridis(3)[1],
      lwd = 2)

# annotation
text(x = speech1_keypoints$frame[speech1_pos_id],
     y = speech1_keypoints$right_wrist_movement[speech1_pos_id],
     labels = "Speech 1",
     pos = 4,
     col = viridis(3)[1])

# Speech 2, right wrist
lines(x = speech2_keypoints$frame,
      y = speech2_keypoints$right_wrist_movement,
      col = viridis(3)[2],
      lwd = 2)

# annotation
speech2_pos_id_r <- which.max(speech2_keypoints$right_wrist_movement)
text(x = speech2_keypoints$frame[speech2_pos_id_r],
     y = speech2_keypoints$right_wrist_movement[speech2_pos_id_r],
     labels = "Speech 2",
     pos = 4,
     col = viridis(3)[2])

```

Our quantification of wrist movement confirms two observations: First, we see more overall hand (wrist) movement in speech 2 than in speech 1. Second, Klaus Ernst gesticulates more strongly with his right hand than with his left hand, resulting in more movement in his right wrist compared to his left wrist.

To see whether those observations are correct, let's have a look at both video sequences again:

![](videos/gifs/speech1_gabriela_heinrich.gif)

![](videos/gifs/speech2_klaus_ernst.gif)


To get our measure of gesticulation, all left to do is to summarize the times series of wrist movement by calculating the average movement of the left and right wrist in both speeches:

```{r}
# Gesticulation speech 1
gesticulations_speech1 <- 
  mean(c(speech1_keypoints$left_wrist_movement,
         speech1_keypoints$right_wrist_movement),
       na.rm = T)

# Gesticulation speech 2
gesticulations_speech2 <- 
  mean(c(speech2_keypoints$left_wrist_movement,
         speech2_keypoints$right_wrist_movement),
       na.rm = T)

# which value should be higher?
gesticulations_speech1
gesticulations_speech2
```

Here is a visual comparison of the result:

```{r}
plot(x = c(gesticulations_speech1,
           gesticulations_speech2),
     y = 2:1,
     xlim = c(-0.005, 0.03),
     ylim = c(0.5, 2.5),
     yaxt = "n",
     xlab = "Gesticulation Indicator",
     ylab = "")
grid()
abline(v = 0,
       lwd = 2)
points(x = c(gesticulations_speech1,
             gesticulations_speech2),
       y = 2:1,
     pch = 19,
     cex = 2,
     col = viridis(1))
text(x = c(gesticulations_speech1,
           gesticulations_speech2),
     y = 2:1,
     labels = c("Speech 1\n(Gabriela Heinrich)",
                "Speech 2\n(Klaus Ernst)"),
     pos = 4,
     col = viridis(1))
```

In line with what we would hope and expect, the gesticulation indicator suggests that Klaus Ernst (speech 2) showed higher levels of gesticulation than Gabriela Heinrich (speech 1).

# Calculating measure of posture

Next, we calculate the measure of posture. While the gesticulation measure quantifies hand movement irrespective of its form, posture quantifies the extent to which speakers adopt body size increasing postures. We quantify this by calculating the average height of the wrist, relative to the shoulder. Our measure of posture is the average height of the higher wrist across all frames in a speech sequence.

We start with a function that calculates the height of the left and right wrist, and determines which of both is higher in any frame of a sequence:

```{r}
calculate_wrist_height <- function(kp_data){
  
  # left shoulder: kp6
  # left wrist: kp10
  # right shoulder: kp7
  # right wrist: kp11
  
  # left height:
  kp_data$left_wrist_height <- kp_data$kp6_y - kp_data$kp10_y
  
  # right wrist height:
  kp_data$right_wrist_height <- kp_data$kp7_y - kp_data$kp11_y
  
  # height of the higher wrist
  kp_data$max_wrist_height <- 
    apply(kp_data[, c("left_wrist_height", "right_wrist_height")], 1, max)
  
  return(kp_data)
}
```

We apply the function:

```{r}
speech1_keypoints <- calculate_wrist_height(speech1_keypoints)
speech2_keypoints <- calculate_wrist_height(speech2_keypoints)
```

Before averaging over all frames, let's take a look at the time series again.

```{r}
par(mfrow = c(1, 2))

# LEFT WRIST
plot(x = speech1_keypoints$frame,
     y = speech1_keypoints$right_wrist_height,
     type = "n",
     las = 1,
     xlab = "Video Frame",
     ylab = "",
     main = "Left Wrist Height",
     ylim = c(-0.27, 0.11),
     xlim = c(0, 450))

grid()

# Speech 1, left wrist
lines(x = speech1_keypoints$frame,
      y = speech1_keypoints$left_wrist_height,
      col = viridis(3)[1],
      lwd = 2)

# annotation
speech1_pos_id <- which.max(speech1_keypoints$frame)-1
text(x = speech1_keypoints$frame[speech1_pos_id],
     y = speech1_keypoints$left_wrist_height[speech1_pos_id],
     labels = "Speech 1",
     pos = 4,
     col = viridis(3)[1])

# Speech 2, left wrist
lines(x = speech2_keypoints$frame,
      y = speech2_keypoints$left_wrist_height,
      col = viridis(3)[2],
      lwd = 2)

# annotation
speech2_pos_id_l <- which.max(speech2_keypoints$left_wrist_height)
text(x = speech2_keypoints$frame[speech2_pos_id_l],
     y = speech2_keypoints$left_wrist_height[speech2_pos_id_l],
     labels = "Speech 2",
     pos = 3,
     col = viridis(3)[2])



# RIGHT WRIST
plot(x = speech1_keypoints$frame,
     y = speech1_keypoints$right_wrist_height,
     type = "n",
     las = 1,
     xlab = "Video Frame",
     ylab = "",
     main = "Right Wrist Height",
     ylim = c(-0.27, 0.11),
     xlim = c(0, 450))

grid()

# Speech 1, right wrist
lines(x = speech1_keypoints$frame,
      y = speech1_keypoints$right_wrist_height,
      col = viridis(3)[1],
      lwd = 2)

# annotation
text(x = speech1_keypoints$frame[speech1_pos_id],
     y = speech1_keypoints$right_wrist_height[speech1_pos_id],
     labels = "Speech 1",
     pos = 4,
     col = viridis(3)[1])

# Speech 2, right wrist
lines(x = speech2_keypoints$frame,
      y = speech2_keypoints$right_wrist_height,
      col = viridis(3)[2],
      lwd = 2)

# annotation
speech2_pos_id_r <- which.max(speech2_keypoints$right_wrist_height)
text(x = speech2_keypoints$frame[speech2_pos_id_r],
     y = speech2_keypoints$right_wrist_height[speech2_pos_id_r],
     labels = "Speech 2",
     pos = 4,
     col = viridis(3)[2])

```

We observe three things: First, while Gabriela Heinrich raises her right wrist to some extent in the middle of the sequence, she never raises her wrists above her shoulders. Second, Klaus Ernst frequently raises both wrist above shoulder height in the first half of the sequence. Third, in the second half of the sequence, Klaus Ernst only raises his right wrist.

We can again confirm these observations by inspecting the videos:

![](videos/gifs/speech1_gabriela_heinrich.gif)

![](videos/gifs/speech2_klaus_ernst.gif)

The summary measure of posture is given by the average height of the higher wrist across all frames of a sequence:

```{r}
posture_speech1 <- mean(speech1_keypoints$max_wrist_height)
posture_speech2 <- mean(speech2_keypoints$max_wrist_height)

# which should be higher?
posture_speech1
posture_speech2
```

Again, we'll visually compare the result of both speeches:

```{r}
plot(x = c(posture_speech1,
           posture_speech2),
     y = 2:1,
     xlim = c(-0.3, 0.05),
     ylim = c(0.5, 2.5),
     yaxt = "n",
     xlab = "Posture Indicator",
     ylab = "")
grid()
abline(v = 0,
       lwd = 2)
points(x = c(posture_speech1,
             posture_speech2),
       y = 2:1,
     pch = 19,
     cex = 2,
     col = viridis(1))
text(x = c(posture_speech1,
           posture_speech2),
     y = 2:1,
     labels = c("Speech 1\n(Gabriela Heinrich)",
                "Speech 2\n(Klaus Ernst)"),
     pos = 2,
     col = viridis(1))

```

Both values are negative, indicating that, on average, both speakers held their wrist below their shoulders. Again, the results confirm our intuition, showing that Klaus Ernst made more use of body-size increasing postures than Gabriela Heinrich.


# Wrapping up

In this workshop, we learned how to apply the pose estimation model `MoveNet` to videos of political speech, explored the resulting data, and calculated measures of gesticulation and posture based on it. 

For a more detailed discussion on the application of pose estimation models for the analysis of politicians' body language during political speech, please refer to the following paper:

  - Rittmann, Oliver (2024). A Measurement Framework for Computationally Analyzing Politicians' Body Language. _OSF Preprint_, available at [doi.org/10.31219/osf.io/9wynp](https://doi.org/10.31219/osf.io/9wynp).





